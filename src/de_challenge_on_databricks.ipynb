{"cells":[{"cell_type":"code","source":["# PART 1: Spark RDD API\n\n## Task 1: Downloads data file and makes it available to Spark\n\nfrom pyspark.sql import SparkSession\nimport os\nimport requests\n\n# Instanciates SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Parameters\ncsv_url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv\"\nfile_name = \"groceries.csv\"\ndbfs_path = \"dbfs:/tmp/input_data/\"\n\n\ndef download_file(url, file_name, dbfs_path):\n    '''\n    Dowloads single file from url and push it to dbfs.\n\n            Parameters:\n                    url (str): url of the site where the file is hosted\n                    file_name (srt): Name of the file\n                    dbfs_path (str): Target path where the file will be saved\n\n            Returns:\n                    target_path (str): path to the saved file\n    '''\n    # gets file from url\n    res = requests.get(url, allow_redirects=True)\n    assert res.status_code == 200, \"Failed to download file: {}\".format(res.text)\n    file_content = res.text\n    # puts file into dbfs\n    target_path = os.path.join(dbfs_path,file_name)\n    dbutils.fs.put(target_path,file_content, overwrite=True)\n    return (target_path)\n\ndef csv_to_df(dbfs_file):\n    '''\n    makes local file available to Spark as pyspark.sql.dataframe.DataFrame.\n\n            Parameters:\n                    dbfs_file (str): Path to source file\n\n            Returns:\n                    DataFrame (str): pyspark.sql.dataframe.DataFrame with csv contents\n    '''\n    out_df = spark.read.format('csv')\\\n       .option('header', 'false')\\\n       .option('inferSchema', 'true')\\\n       .load(dbfs_file)\n    return out_df\n\n# downloads csv file into dbfs\ndbfs_file = download_file(csv_url, file_name, dbfs_path)\n\n# loads file into dataframe\ngroceries_df = csv_to_df(dbfs_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 1 - Task 1","showTitle":true,"inputWidgets":{},"nuid":"a044a002-8660-40cb-815c-8a4b0a9adec5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 1: Spark RDD API\n\n## Task 2 - Part a: unique list of products\n\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\ndef file_exists(path):\n  try:\n    dbutils.fs.ls(path)\n    return True\n  except Exception as e:\n    if 'java.io.FileNotFoundException' in str(e):\n      return False\n    else:\n      raise\n      \ndef create_pair(item): \n    return (item, 1) \n\n# returns list of pairs with unique key (product name), and value set as 1\ngroceries_summary = groceries_df.rdd\\\n    .flatMap(list)\\\n    .map(create_pair)\\\n    .reduceByKey(lambda a,b: a+b)\\\n    .filter(lambda x: x[0] != None)\\\n\nunique_products = groceries_summary.keys()\n\n\n# all output files are placed in the same dir\noutput_path = \"dbfs:/tmp/out\"\ndbutils.fs.mkdirs(output_path)\n\n# list of unique products\nfilename = os.path.join(output_path,\"out_1_2a.txt\")\nif file_exists is not True:\n  unique_products.coalesce(1).saveAsTextFile(filename)\n\n# count of total items\nfilename = os.path.join(output_path,\"out_1_2b.txt\")\ntotal_items = sum(groceries_summary.values().collect())\n\n# creates spark dataframe with column name \"count\" and writes it to file\nschema = StructType([StructField(\"count\",IntegerType(),True)])  \ndata = [(total_items,)]\ntotal_items_df = spark.createDataFrame(data=data, schema=schema)\ntotal_items_df.select(\"count\").coalesce(1)\\\n    .write\\\n    .mode (\"overwrite\")\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .save(filename)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 1 - Task 2","showTitle":true,"inputWidgets":{},"nuid":"ac484a75-4a9f-404c-a8c3-7b96cf15afec"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 1: Spark RDD API\n\n## Task 3: Top 5 products\n\nfilename = os.path.join(output_path,\"out_1_3.txt\")\ntop5groceries = groceries_summary.takeOrdered(5,lambda x: -x[1])\n\n# creates spark dataframe with column name \"count\" and writes it to file\nschema = StructType([StructField(\"product\",StringType(),True),\\\n                     StructField(\"count\",IntegerType(),True)])  \ntop5groceries_df = spark.createDataFrame(data=top5groceries, schema=schema)\ntop5groceries_df.coalesce(1)\\\n    .write\\\n    .mode (\"overwrite\")\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .save(filename)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 1 - Task 3","showTitle":true,"inputWidgets":{},"nuid":"ff469b6a-ef39-4c27-b778-25f1aa96b67f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 2: Spark Dataframe API\n\n## Task 1: Downloads parquet file and make it available to Spark\n\nfrom pyspark import SparkContext,SparkFiles\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType\nimport os\n\n# instanciates spark session object\nspark = SparkSession.builder.getOrCreate()\n\nurl = \"https://github.com/databricks/LearningSparkV2/blob/master/mlflow-project-example/data/sf-airbnb-clean.parquet/\"\nfile_path = \"dbfs:/FileStore/tables/sf-airbnb-clean/\"\noutput_path = \"dbfs:/tmp/out\"\ndbutils.fs.mkdirs(output_path)\n\nairbnb_df = spark.read.parquet(file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 2 - Task 1","showTitle":true,"inputWidgets":{},"nuid":"74d97d97-7cfa-4444-a899-f25aee5037d1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 2: Spark Dataframe API\n\n## Task 2: Creates CSV that lists the minimum price, maximum price and total row count\n\nsummary = airbnb_df\\\n    .groupBy()\\\n    .min('price')\\\n    .collect()[0]\\\n    .__getitem__(0)\\\n,airbnb_df\\\n    .groupBy()\\\n    .max('price')\\\n    .collect()[0]\\\n    .__getitem__(0)\\\n,airbnb_df.count()\n\nfilename = os.path.join(output_path,\"out_2_2.txt\")\nschema = StructType([StructField(\"min_price\",FloatType(),True),\\\n                     StructField(\"max_price\",FloatType(),True),\\\n                     StructField(\"total\",IntegerType(),True)])  \n\nsummary_df = spark.createDataFrame(data=[summary], schema=schema)\nsummary_df.coalesce(1)\\\n    .write\\\n    .mode (\"overwrite\")\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .save(filename)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 2 - Task 2","showTitle":true,"inputWidgets":{},"nuid":"64fa3672-1b23-4ed4-8369-8b32666db692"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 2: Spark Dataframe API\n\n## Task 3: Calculate the average number of bathrooms and bedrooms across all the properties listed in this data set with a price of > 5000 and a review score being exactly equalt to 10.\n\nselected_df = airbnb_df\\\n    .filter((airbnb_df.price>5000 ) & (airbnb_df.review_scores_value==10))\n\navg_summary = selected_df\\\n    .groupBy()\\\n    .avg('bathrooms')\\\n    .collect()[0]\\\n    .__getitem__(0)\\\n,airbnb_df\\\n    .groupBy()\\\n    .avg('bedrooms')\\\n    .collect()[0]\\\n    .__getitem__(0)\n\n\nfilename = os.path.join(output_path,\"out_2_3.txt\")\nschema = StructType([StructField(\"bathrooms\",FloatType(),True),\\\n                     StructField(\"bedrooms\",FloatType(),True)])  \navg_summary_df = spark.createDataFrame(data=[avg_summary], schema=schema)\navg_summary_df.coalesce(1)\\\n    .write\\\n    .mode (\"overwrite\")\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .save(filename)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 2 - Task 3","showTitle":true,"inputWidgets":{},"nuid":"45c87380-174a-40c1-9e3c-7ba43e7d4938"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 2: Spark Dataframe API\n\n## Task 4: How many people can be accomodated by the property with the lowest price and highest rating?\n\nnaccomodates_bestdeal = airbnb_df\\\n    .orderBy(airbnb_df.price.asc(),airbnb_df.review_scores_rating.desc())\\\n    .select('accommodates')\\\n    .take(1)[0]\\\n    .__getitem__(0)\n\nfilename = os.path.join(output_path,\"out_2_4.txt\")\nschema = StructType([StructField(\"n_people\",FloatType(),True)])\ndata = [(naccomodates_bestdeal,)]\nnaccomodates_bestdeal_df = spark.createDataFrame(data=data, schema=schema)\nnaccomodates_bestdeal_df.coalesce(1)\\\n    .write\\\n    .mode (\"overwrite\")\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .save(filename)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 2 - Task 4","showTitle":true,"inputWidgets":{},"nuid":"d813a4be-2983-411c-a657-d611ba1309d9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh pip install \"apache-airflow[databricks]\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 2 - Task 5","showTitle":true,"inputWidgets":{},"nuid":"3ffee912-a75a-45ab-97bb-2b7fed1b63cd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 2: Spark Dataframe API\n\n## Task 5: Apache Airflow\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom datetime import datetime, timedelta\n\n# The next section sets some default arguments applied to each task in the DAG\nargs = {\n    'owner': 'airflow',\n    'email': ['airflow@example.com'],\n    'depends_on_past': False,\n    'start_date': airflow.utils.dates.days_ago(0)\n}\n\n#The DAG instantiation statement gives the DAG a unique ID, attaches the default arguments, and gives it a daily schedule (example).\ndag = DAG(dag_id='Task_2_5_DAG', default_args=args, schedule_interval='@daily')\n\ntask1 = DummyOperator(task_id='Task_1', dag=dag)\ntask2 = DummyOperator(task_id='Task_2', dag=dag)\ntask3 = DummyOperator(task_id='Task_3', dag=dag)\ntask4 = DummyOperator(task_id='Task_4', dag=dag)\ntask5 = DummyOperator(task_id='Task_5', dag=dag)\ntask6 = DummyOperator(task_id='Task_6', dag=dag)\n\ntask1 >> [task2, task3]\ntask2 >> [task4, task5, task6]\ntask3 >> [task4, task5, task6]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 2 - Task 5","showTitle":false,"inputWidgets":{},"nuid":"aa83f414-4ff1-40be-a210-628e18b0c5f3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[" %sh curl -L \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" -o \"/tmp/iris.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 3 - Task 1","showTitle":true,"inputWidgets":{},"nuid":"8a88c2bd-6170-40e5-ac8b-dcbdd6044e2a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PART 3: Applied ML\n\n## Task 1 sklearn\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nlocal_file = \"file:/tmp/iris.csv\"\ndf = pd.read_csv(local_file,\\\n    names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n\n# Separate features from class.\narray = df.values\nX = array[:,0:4]\ny = array[:,4]\n\n# Fit Logistic Regression classifier.\nlogreg = LogisticRegression(C=1e5)\nlogreg.fit(X, y)\n\n# Predict on training data. Seems to work.\n# 5.1     3.5     1.4     0.2     Iris-setosa\n# 6.2     3.4     5.4     2.3     Iris-virginica\nprint(logreg.predict([[5.1, 3.5, 1.4, 0.2]]))\nprint(logreg.predict([[6.2, 3.4, 5.4, 2.3]]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 3 - Task 1","showTitle":false,"inputWidgets":{},"nuid":"bab164f4-8162-4ed6-9ffd-4fae7ca56b33"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;Iris-setosa&#39;]\n[&#39;Iris-virginica&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Iris-setosa&#39;]\n[&#39;Iris-virginica&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# PART 3: Applied ML\n\n## Task 2: MLlib\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, IndexToString\nfrom pyspark.sql import SparkSession\nimport os\n\n\n# Instanciates SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# all output files are placed in the same dir\noutput_path = \"dbfs:/tmp/out\"\ndbutils.fs.mkdirs(output_path)\n\n# uploads downloaded file into dbfs\nlocal_file = \"file:/tmp/iris.csv\"\ndbfs_file = \"dbfs:/tmp/input_data/iris.csv\"\ndbutils.fs.cp(local_file,dbfs_file)\n\ncol_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n\nschema = \"\"\"`sepal_length` DOUBLE,\n            `sepal_width` DOUBLE,\n            `petal_length` DOUBLE,\n            `petal_width` DOUBLE,\n            `class` STRING\n        \"\"\"\n\ndf = spark.read.csv(dbfs_file,schema=schema)\n    \ncategoricalCols = [\"class\"]\n\n# The following two lines are estimators. They return functions that we will later apply to transform the dataset.\n\n# Convert it to a numeric value using StringIndexer.\nlabelToIndex = StringIndexer(inputCol=\"class\", outputCol=\"indexed_class\")\nlabelIndexer = labelToIndex.fit(df)\nlabelReverser = IndexToString(inputCol=\"prediction\", outputCol=\"class\", labels=labelIndexer.labels)\n\n# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\nnumericCols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n\nvecAssembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"indexed_class\", regParam=1e5)\n\n# Define the pipeline based on the stages created in previous steps.\npipeline = Pipeline(stages=[labelToIndex, vecAssembler, lr, labelReverser])\n\n# Define the pipeline model.\npipelineModel = pipeline.fit(df)\n\ntest_df = spark.createDataFrame([\n    (5.1, 3.5, 1.4, 0.2),\n    (6.2, 3.4, 5.4, 2.3)\n], [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n\n# Apply the pipeline model to the test dataset.\npred_df = pipelineModel.transform(test_df)\n\nfilename = os.path.join(output_path,\"out_3_2.txt\")\npred_df.select(\"class\").coalesce(1)\\\n    .write\\\n    .mode (\"overwrite\")\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .save(filename)\n        \npred_df.select(\"class\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Part 3 - Task 2","showTitle":true,"inputWidgets":{},"nuid":"7ea08cd1-86a4-4e00-946c-0b6dd89a0d48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------+\n|         class|\n+--------------+\n|   Iris-setosa|\n|Iris-virginica|\n+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+\n         class|\n+--------------+\n   Iris-setosa|\nIris-virginica|\n+--------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["file_name = \"out_3_2.txt\"\noutput_path = \"dbfs:/tmp/out\"\noutput_file = os.path.join(output_path,file_name)\noutput = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\",\"true\")\\\n  .csv(output_file)\ndisplay(output)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Output files - Result Check","showTitle":true,"inputWidgets":{},"nuid":"3386f36f-c43d-4780-8ecc-7161ff22c1bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Iris-setosa"],["Iris-virginica"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"class","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>class</th></tr></thead><tbody><tr><td>Iris-setosa</td></tr><tr><td>Iris-virginica</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"de_challenge_on_databricks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":859507003237801}},"nbformat":4,"nbformat_minor":0}
